{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJHlxbR5kEe-"
   },
   "source": [
    "# Mixed Logit with correlations\n",
    "\n",
    "Using swissmetro data, comparing results to biogeme (Bierlaire, M. (2018). PandasBiogeme: a short introduction. EPFL (Transport and Mobility Laboratory, ENAC))\n",
    "\n",
    "\n",
    "So mnl is identical. mixed param for travel time too, for both with and w/o panel. MXL for both cost and time random is also identical for both w and w/o panel.\n",
    "\n",
    "It's really just a problem with ECs. Actually, that's because they are not identified. Three alternatives, so only one sd is identified. one is zero and is 1. garh. for correlated vars, one off-diag of all three would be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:33.124850Z",
     "iopub.status.busy": "2025-07-08T19:56:33.124047Z",
     "iopub.status.idle": "2025-07-08T19:56:34.342471Z",
     "shell.execute_reply": "2025-07-08T19:56:34.341229Z",
     "shell.execute_reply.started": "2025-07-08T19:56:33.124820Z"
    },
    "id": "NQbZt7CVh8f_",
    "outputId": "b823e80f-fd47-4dd1-8656-3fd0d6a1e26a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jaxlogit.mixed_logit import MixedLogit\n",
    "from jaxlogit.utils import wide_to_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:34.344790Z",
     "iopub.status.busy": "2025-07-08T19:56:34.343922Z",
     "iopub.status.idle": "2025-07-08T19:56:34.351467Z",
     "shell.execute_reply": "2025-07-08T19:56:34.349531Z",
     "shell.execute_reply.started": "2025-07-08T19:56:34.344752Z"
    }
   },
   "outputs": [],
   "source": [
    "#  64bit precision\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP77ezqVfvRI"
   },
   "source": [
    "## Swissmetro Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:34.353815Z",
     "iopub.status.busy": "2025-07-08T19:56:34.353170Z",
     "iopub.status.idle": "2025-07-08T19:56:38.262432Z",
     "shell.execute_reply": "2025-07-08T19:56:38.260660Z",
     "shell.execute_reply.started": "2025-07-08T19:56:34.353766Z"
    },
    "id": "4jqERhnWhGCc",
    "outputId": "6bbdca2a-1670-4836-c0d5-d16915ee9597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding 0 observations from 6768 total observations.\n"
     ]
    }
   ],
   "source": [
    "df_wide = pd.read_table(\"http://transp-or.epfl.ch/data/swissmetro.dat\", sep='\\t')\n",
    "\n",
    "# Keep only observations for commute and business purposes that contain known choices\n",
    "df_wide = df_wide[(df_wide['PURPOSE'].isin([1, 3]) & (df_wide['CHOICE'] != 0))]\n",
    "\n",
    "df_wide['custom_id'] = np.arange(len(df_wide))  # Add unique identifier\n",
    "df_wide['CHOICE'] = df_wide['CHOICE'].map({1: 'TRAIN', 2:'SM', 3: 'CAR'})\n",
    "\n",
    "# biogeme data prep\n",
    "#exclude = ((PURPOSE != 1) * (PURPOSE != 3) + (CHOICE == 0)) > 0\n",
    "exclude = ((df_wide['PURPOSE'] != 1) * (df_wide['PURPOSE'] != 3) + (df_wide['CHOICE'] == 0)) > 0\n",
    "print(f\"Excluding {exclude.sum()} observations from {len(df_wide)} total observations.\")\n",
    "df_wide = df_wide[~exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:38.265555Z",
     "iopub.status.busy": "2025-07-08T19:56:38.264981Z",
     "iopub.status.idle": "2025-07-08T19:56:38.276569Z",
     "shell.execute_reply": "2025-07-08T19:56:38.274714Z",
     "shell.execute_reply.started": "2025-07-08T19:56:38.265418Z"
    }
   },
   "outputs": [],
   "source": [
    "#TRAIN_AV_SP = database.define_variable('TRAIN_AV_SP', TRAIN_AV * (SP != 0))\n",
    "df_wide['TRAIN_AV'] = df_wide['TRAIN_AV'] * (df_wide['SP'] != 0)\n",
    "# CAR_AV_SP = database.define_variable('CAR_AV_SP', CAR_AV * (SP != 0))\n",
    "df_wide['CAR_AV'] = df_wide['CAR_AV'] * (df_wide['SP'] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:38.279703Z",
     "iopub.status.busy": "2025-07-08T19:56:38.278357Z",
     "iopub.status.idle": "2025-07-08T19:56:38.320593Z",
     "shell.execute_reply": "2025-07-08T19:56:38.319311Z",
     "shell.execute_reply.started": "2025-07-08T19:56:38.279652Z"
    },
    "id": "1KM-BvFvhWed",
    "outputId": "33a6bacf-9674-4fec-eeca-90ab763a3308"
   },
   "outputs": [],
   "source": [
    "df = wide_to_long(df_wide, id_col='custom_id', alt_name='alt', sep='_',\n",
    "                  alt_list=['TRAIN', 'SM', 'CAR'], empty_val=0,\n",
    "                  varying=['TT', 'CO', 'HE', 'AV', 'SEATS'], alt_is_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:38.322509Z",
     "iopub.status.busy": "2025-07-08T19:56:38.321989Z",
     "iopub.status.idle": "2025-07-08T19:56:38.357534Z",
     "shell.execute_reply": "2025-07-08T19:56:38.356358Z",
     "shell.execute_reply.started": "2025-07-08T19:56:38.322408Z"
    },
    "id": "MsSu2jqKeoz-"
   },
   "outputs": [],
   "source": [
    "df['ASC_TRAIN'] = np.where(df['alt'] == 'TRAIN', 1, 0)\n",
    "df['ASC_CAR'] = np.where(df['alt'] == 'CAR', 1, 0)\n",
    "df['ASC_SM'] = np.where(df['alt'] == 'SM', 1, 0)\n",
    "\n",
    "df['TT'] = df['TT'] / 100.0\n",
    "df['CO'] = df['CO'] / 100.0\n",
    "\n",
    "annual_pass = (df['GA'] == 1) & (df['alt'].isin(['TRAIN', 'SM']))\n",
    "df.loc[annual_pass, 'CO'] = 0  # Cost zero for pass holders\n",
    "\n",
    "## Now above before long to wide\n",
    "#CAR_AV_SP = database.define_variable('CAR_AV_SP', CAR_AV * (SP != 0))\n",
    "#TRAIN_AV_SP = database.define_variable('TRAIN_AV_SP', TRAIN_AV * (SP != 0))\n",
    "#df.loc[(df['SP'] == 0) & (df['alt'].isin(['CAR', 'TRAIN'])), 'AV'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:56:38.359413Z",
     "iopub.status.busy": "2025-07-08T19:56:38.358828Z",
     "iopub.status.idle": "2025-07-08T19:56:38.392533Z",
     "shell.execute_reply": "2025-07-08T19:56:38.390854Z",
     "shell.execute_reply.started": "2025-07-08T19:56:38.359386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(6768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.custom_id.unique().max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so setting sd of car and train to zero means we get the same result. WTF is this important? Aren't they all relative? Actually, no. What happens for panel data? All are identified, it seems.\n",
    "# try different fixed sd.car: record init LL and solution. Looks like solution is the same for 0, 0.4, and 1? double-check! simply means it is not identified, but why the big difference for bg? num draws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T21:14:31.628831Z",
     "iopub.status.busy": "2025-07-08T21:14:31.627903Z",
     "iopub.status.idle": "2025-07-08T21:14:31.636024Z",
     "shell.execute_reply": "2025-07-08T21:14:31.634301Z",
     "shell.execute_reply.started": "2025-07-08T21:14:31.628788Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.coeff_names\n",
    "init_vals = None  #[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T21:14:33.355042Z",
     "iopub.status.busy": "2025-07-08T21:14:33.353972Z",
     "iopub.status.idle": "2025-07-08T21:14:54.018002Z",
     "shell.execute_reply": "2025-07-08T21:14:54.016938Z",
     "shell.execute_reply.started": "2025-07-08T21:14:33.354995Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:07:05,755 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 2000 random draws for each random variable and observation.\n",
      "INFO:2025-07-13 20:07:06,255:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-07-13 20:07:06,255 INFO jax._src.xla_bridge: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-07-13 20:07:13,601 INFO jaxlogit.mixed_logit: Shape of draws: (6768, 3, 2000), number of draws: 2000\n",
      "2025-07-13 20:07:13,602 INFO jaxlogit.mixed_logit: Shape of Xdf: (6768, 2, 2), shape of Xdr: (6768, 2, 3)\n",
      "2025-07-13 20:07:13,603 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
      "2025-07-13 20:07:13,961 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 7017.40, params= [(np.str_('ASC_SM'), Array(0.1, dtype=float64)), (np.str_('ASC_CAR'), Array(0.1, dtype=float64)), (np.str_('ASC_TRAIN'), Array(0., dtype=float64)), (np.str_('TT'), Array(0.1, dtype=float64)), (np.str_('CO'), Array(0.1, dtype=float64)), (np.str_('sd.ASC_SM'), Array(0.1, dtype=float64)), (np.str_('sd.ASC_CAR'), Array(0.1, dtype=float64)), (np.str_('sd.ASC_TRAIN'), Array(0.1, dtype=float64))]\n",
      "2025-07-13 20:07:13,962 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 7017.403837161417, Loss on the last accepted step: 0.0, Step size: 1.0\n",
      "Loss on this step: 1788933.551879583, Loss on the last accepted step: 7017.403837161417, Step size: 0.25\n",
      "Loss on this step: 1112431.1853748779, Loss on the last accepted step: 7017.403837161417, Step size: 0.0625\n",
      "Loss on this step: 304294.42034264264, Loss on the last accepted step: 7017.403837161417, Step size: 0.015625\n",
      "Loss on this step: 84235.30877935124, Loss on the last accepted step: 7017.403837161417, Step size: 0.00390625\n",
      "Loss on this step: 21000.80765728151, Loss on the last accepted step: 7017.403837161417, Step size: 0.0009765625\n",
      "Loss on this step: 6411.406752406871, Loss on the last accepted step: 7017.403837161417, Step size: 0.0009765625\n",
      "Loss on this step: 5470.330609456536, Loss on the last accepted step: 6411.406752406871, Step size: 0.0009765625\n",
      "Loss on this step: 5399.815340893587, Loss on the last accepted step: 5470.330609456536, Step size: 0.0009765625\n",
      "Loss on this step: 5368.082837629614, Loss on the last accepted step: 5399.815340893587, Step size: 0.0009765625\n",
      "Loss on this step: 5359.756898907877, Loss on the last accepted step: 5368.082837629614, Step size: 0.0009765625\n",
      "Loss on this step: 5353.8682962772455, Loss on the last accepted step: 5359.756898907877, Step size: 0.00341796875\n",
      "Loss on this step: 5339.136622675173, Loss on the last accepted step: 5353.8682962772455, Step size: 0.011962890625\n",
      "Loss on this step: 5322.1483047251295, Loss on the last accepted step: 5339.136622675173, Step size: 0.011962890625\n",
      "Loss on this step: 5341.462398582338, Loss on the last accepted step: 5322.1483047251295, Step size: 0.00299072265625\n",
      "Loss on this step: 5320.031265553987, Loss on the last accepted step: 5322.1483047251295, Step size: 0.00299072265625\n",
      "Loss on this step: 5315.637575075568, Loss on the last accepted step: 5320.031265553987, Step size: 0.00299072265625\n",
      "Loss on this step: 5311.954158558539, Loss on the last accepted step: 5315.637575075568, Step size: 0.00299072265625\n",
      "Loss on this step: 5309.300873384235, Loss on the last accepted step: 5311.954158558539, Step size: 0.00299072265625\n",
      "Loss on this step: 5304.963106475, Loss on the last accepted step: 5309.300873384235, Step size: 0.010467529296875\n",
      "Loss on this step: 5291.400480009685, Loss on the last accepted step: 5304.963106475, Step size: 0.0366363525390625\n",
      "Loss on this step: 5284.65907515949, Loss on the last accepted step: 5291.400480009685, Step size: 0.0366363525390625\n",
      "Loss on this step: 5267.7658688391175, Loss on the last accepted step: 5284.65907515949, Step size: 0.0366363525390625\n",
      "Loss on this step: 5262.310539109814, Loss on the last accepted step: 5267.7658688391175, Step size: 0.0366363525390625\n",
      "Loss on this step: 5259.571481509835, Loss on the last accepted step: 5262.310539109814, Step size: 0.0366363525390625\n",
      "Loss on this step: 5258.700017262037, Loss on the last accepted step: 5259.571481509835, Step size: 0.0366363525390625\n",
      "Loss on this step: 5258.356509585759, Loss on the last accepted step: 5258.700017262037, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.891334849217, Loss on the last accepted step: 5258.356509585759, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.830860794536, Loss on the last accepted step: 5257.891334849217, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.640699246114, Loss on the last accepted step: 5257.830860794536, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.54717111804, Loss on the last accepted step: 5257.640699246114, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.381716021293, Loss on the last accepted step: 5257.54717111804, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.281436500571, Loss on the last accepted step: 5257.381716021293, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.205793203164, Loss on the last accepted step: 5257.281436500571, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.106448669038, Loss on the last accepted step: 5257.205793203164, Step size: 0.0366363525390625\n",
      "Loss on this step: 5257.0398764627125, Loss on the last accepted step: 5257.106448669038, Step size: 0.12822723388671875\n",
      "Loss on this step: 5256.815730337536, Loss on the last accepted step: 5257.0398764627125, Step size: 0.4487953186035156\n",
      "Loss on this step: 5256.3107762646705, Loss on the last accepted step: 5256.815730337536, Step size: 1.5707836151123047\n",
      "Loss on this step: 5257.793101290949, Loss on the last accepted step: 5256.3107762646705, Step size: 0.39269590377807617\n",
      "Loss on this step: 5256.228917746596, Loss on the last accepted step: 5256.3107762646705, Step size: 0.39269590377807617\n",
      "Loss on this step: 5255.839714247547, Loss on the last accepted step: 5256.228917746596, Step size: 0.39269590377807617\n",
      "Loss on this step: 5255.727798092415, Loss on the last accepted step: 5255.839714247547, Step size: 1.3744356632232666\n",
      "Loss on this step: 5255.605248128341, Loss on the last accepted step: 5255.727798092415, Step size: 4.810524821281433\n",
      "Loss on this step: 5255.533833399881, Loss on the last accepted step: 5255.605248128341, Step size: 16.836836874485016\n",
      "Loss on this step: 5255.499498385495, Loss on the last accepted step: 5255.533833399881, Step size: 58.928929060697556\n",
      "Loss on this step: 5255.480770678838, Loss on the last accepted step: 5255.499498385495, Step size: 206.25125171244144\n",
      "Loss on this step: 5255.472410378204, Loss on the last accepted step: 5255.480770678838, Step size: 721.879380993545\n",
      "Loss on this step: 5255.467418243547, Loss on the last accepted step: 5255.472410378204, Step size: 2526.5778334774077\n",
      "Loss on this step: 5255.465083568514, Loss on the last accepted step: 5255.467418243547, Step size: 8843.022417170927\n",
      "Loss on this step: 5255.463849467192, Loss on the last accepted step: 5255.465083568514, Step size: 30950.578460098244\n",
      "Loss on this step: 5255.463200350106, Loss on the last accepted step: 5255.463849467192, Step size: 108327.02461034385\n",
      "Loss on this step: 5255.46285643133, Loss on the last accepted step: 5255.463200350106, Step size: 379144.5861362035\n",
      "Loss on this step: 5255.462669739296, Loss on the last accepted step: 5255.46285643133, Step size: 1327006.0514767123\n",
      "Loss on this step: 5255.462566332029, Loss on the last accepted step: 5255.462669739296, Step size: 4644521.180168493\n",
      "Loss on this step: 5255.462508328542, Loss on the last accepted step: 5255.462566332029, Step size: 16255824.130589724\n",
      "Loss on this step: 5255.4624755617715, Loss on the last accepted step: 5255.462508328542, Step size: 56895384.45706403\n",
      "Loss on this step: 5255.462457191315, Loss on the last accepted step: 5255.4624755617715, Step size: 199133845.5997241\n",
      "Loss on this step: 5255.462447106246, Loss on the last accepted step: 5255.462457191315, Step size: 696968459.5990344\n",
      "Loss on this step: 5255.462441717667, Loss on the last accepted step: 5255.462447106246, Step size: 2439389608.5966206\n",
      "Loss on this step: 5255.46243890935, Loss on the last accepted step: 5255.462441717667, Step size: 8537863630.088172\n",
      "Loss on this step: 5255.462437471791, Loss on the last accepted step: 5255.46243890935, Step size: 29882522705.3086\n",
      "Loss on this step: 5255.462436743919, Loss on the last accepted step: 5255.462437471791, Step size: 104588829468.58011\n",
      "Loss on this step: 5255.462436377611, Loss on the last accepted step: 5255.462436743919, Step size: 366060903140.0304\n",
      "Loss on this step: 5255.462436193856, Loss on the last accepted step: 5255.462436377611, Step size: 1281213160990.1064\n",
      "Loss on this step: 5255.462436101827, Loss on the last accepted step: 5255.462436193856, Step size: 4484246063465.373\n",
      "Loss on this step: 5255.4624360557755, Loss on the last accepted step: 5255.462436101827, Step size: 15694861222128.805\n",
      "Loss on this step: 5255.462436032739, Loss on the last accepted step: 5255.4624360557755, Step size: 54932014277450.81\n",
      "Loss on this step: 5255.462436021218, Loss on the last accepted step: 5255.462436032739, Step size: 192262049971077.84\n",
      "Loss on this step: 5255.4624360154585, Loss on the last accepted step: 5255.462436021218, Step size: 672917174898772.5\n",
      "Loss on this step: 5255.462436012577, Loss on the last accepted step: 5255.4624360154585, Step size: 2355210112145704.0\n",
      "Loss on this step: 5255.4624360111375, Loss on the last accepted step: 5255.462436012577, Step size: 8243235392509964.0\n",
      "Loss on this step: 5255.462436010417, Loss on the last accepted step: 5255.4624360111375, Step size: 2.885132387378487e+16\n",
      "Loss on this step: 5255.462436010059, Loss on the last accepted step: 5255.462436010417, Step size: 1.0097963355824706e+17\n",
      "Loss on this step: 5255.462436009877, Loss on the last accepted step: 5255.462436010059, Step size: 3.534287174538647e+17\n",
      "Loss on this step: 5255.462436009789, Loss on the last accepted step: 5255.462436009877, Step size: 1.2370005110885263e+18\n",
      "Loss on this step: 5255.462436009742, Loss on the last accepted step: 5255.462436009789, Step size: 4.329501788809842e+18\n",
      "Loss on this step: 5255.4624360097205, Loss on the last accepted step: 5255.462436009742, Step size: 1.5153256260834447e+19\n",
      "Loss on this step: 5255.462436009709, Loss on the last accepted step: 5255.4624360097205, Step size: 5.303639691292057e+19\n",
      "Loss on this step: 5255.462436009702, Loss on the last accepted step: 5255.462436009709, Step size: 1.85627389195222e+20\n",
      "Loss on this step: 5255.4624360097, Loss on the last accepted step: 5255.462436009702, Step size: 6.49695862183277e+20\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.4624360097, Step size: 2.2739355176414693e+21\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 5.684838794103673e+20\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 1.4212096985259183e+20\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 3.553024246314796e+19\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 8.88256061578699e+18\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 2.2206401539467474e+18\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 5.5516003848668685e+17\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 1.3879000962167171e+17\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 3.469750240541793e+16\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 8674375601354482.0\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 2168593900338620.5\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 542148475084655.1\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 135537118771163.78\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 33884279692790.945\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 8471069923197.736\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 2117767480799.434\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 529441870199.8585\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 132360467549.96463\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 33090116887.491158\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 8272529221.872789\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 2068132305.4681973\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 517033076.36704934\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 129258269.09176233\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 32314567.272940584\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 8078641.818235146\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 2019660.4545587865\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 504915.1136396966\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 126228.77840992415\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 31557.19460248104\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 7889.29865062026\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 1972.324662655065\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 493.0811656637662\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 123.27029141594156\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 30.81757285398539\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 7.704393213496347\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 1.9260983033740868\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 0.4815245758435217\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 0.12038114396088043\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 0.030095285990220107\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 0.007523821497555027\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 0.0018809553743887567\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 0.00047023884359718917\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 0.00011755971089929729\n",
      "Loss on this step: 5255.4624360097, Loss on the last accepted step: 5255.462436009698, Step size: 2.9389927724824323e-05\n",
      "Loss on this step: 5255.462436009699, Loss on the last accepted step: 5255.462436009698, Step size: 7.347481931206081e-06\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 1.8368704828015202e-06\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 4.5921762070038005e-07\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 1.1480440517509501e-07\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 2.8701101293773753e-08\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 7.175275323443438e-09\n",
      "Loss on this step: 5255.462436009698, Loss on the last accepted step: 5255.462436009698, Step size: 7.175275323443438e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:07:49,326 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -5255.46, final gradient max = 4.50e-10, norm = 5.58e-10.\n",
      "2025-07-13 20:07:49,327 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n",
      "2025-07-13 20:07:54,089 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
      "2025-07-13 20:08:02,672 INFO jaxlogit._choice_model: Post fit processing\n",
      "/home/jzill/code/jaxlogit/jaxlogit/_choice_model.py:104: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self.stderr = np.sqrt(np.diag(self.covariance))\n",
      "2025-07-13 20:08:03,109 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Message: \n",
      "    Iterations: 75\n",
      "    Function evaluations: 131\n",
      "Estimation time= 57.3 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
      "---------------------------------------------------------------------------\n",
      "ASC_SM                  1.2545633     0.1088799    11.5224533      1.95e-30 ***\n",
      "ASC_CAR                 0.7547490     0.0580589    12.9997116      3.51e-38 ***\n",
      "ASC_TRAIN               0.0000000     0.0000000           nan           nan    \n",
      "TT                     -1.7130784     0.0967915   -17.6986446      1.48e-68 ***\n",
      "CO                     -1.7638536     0.1023472   -17.2340277      3.57e-65 ***\n",
      "sd.ASC_SM               3.1599420     0.2653872    11.9069109      2.29e-32 ***\n",
      "sd.ASC_CAR            -24.7148028 826654.7284579    -0.0000299             1    \n",
      "sd.ASC_TRAIN          -25.2825273           nan           nan           nan    \n",
      "---------------------------------------------------------------------------\n",
      "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Log-Likelihood= -5255.462\n",
      "AIC= 10524.925\n",
      "BIC= 10572.665\n"
     ]
    }
   ],
   "source": [
    "varnames = ['ASC_SM', 'ASC_CAR', 'ASC_TRAIN', 'TT', 'CO']\n",
    "\n",
    "#randvars = {'CO': 'n', 'TT': 'n'}  \n",
    "randvars = {'ASC_SM': 'n', 'ASC_CAR': 'n', 'ASC_TRAIN': 'n'}\n",
    "\n",
    "# train is smallest when none are ifxed -> has to be smallest\n",
    "fixedvars = {'ASC_TRAIN': 0.0}  #, 'corr.TT.CO': 0.0}  #, 'sd.ASC_TRAIN': 0.0, 'sd.ASC_CAR': 0.0}  #}  #, 'sd.ASC_CAR': 1.0}  # Mean and variance of SM is fixed to 0, 'sd.ASC_CAR': 0.437869\n",
    "\n",
    "do_panel = False\n",
    "\n",
    "model = MixedLogit()\n",
    "res = model.fit(\n",
    "    X=df[varnames],\n",
    "    y=df['CHOICE'],\n",
    "    varnames=varnames,\n",
    "    alts=df['alt'],\n",
    "    ids=df['custom_id'],\n",
    "    avail=df['AV'],\n",
    "    panels=None if do_panel is False else df[\"ID\"],\n",
    "    randvars=randvars,\n",
    "    n_draws=2000,\n",
    "    fixedvars=fixedvars,\n",
    "    init_coeff=init_vals,\n",
    "    include_correlations=False,  # Enable correlation between random parameters\n",
    "    optim_method='trust-region',  # 'L-BFGS-B'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coeff_names': array(['ASC_SM', 'ASC_CAR', 'ASC_TRAIN', 'TT', 'CO', 'sd.ASC_SM',\n",
       "        'sd.ASC_CAR', 'sd.ASC_TRAIN'], dtype='<U12'),\n",
       " 'coeff_': Array([  1.25456326,   0.75474899,   0.        ,  -1.71307838,\n",
       "         -1.76385362,   3.15994199, -24.71480277, -25.28252733],      dtype=float64),\n",
       " 'stderr': array([1.08879874e-01, 5.80589022e-02, 0.00000000e+00, 9.67915016e-02,\n",
       "        1.02347150e-01, 2.65387221e-01, 8.26654728e+05,            nan]),\n",
       " 'zvalues': Array([ 1.15224533e+01,  1.29997116e+01,             nan, -1.76986446e+01,\n",
       "        -1.72340277e+01,  1.19069109e+01, -2.98973706e-05,             nan],      dtype=float64),\n",
       " 'pvalues': array([1.95230050e-30, 3.50923426e-38,            nan, 1.47883763e-68,\n",
       "        3.57463603e-65, 2.28920264e-32, 9.99976146e-01,            nan]),\n",
       " 'loglikelihood': Array(-5255.46243601, dtype=float64),\n",
       " 'total_fun_eval': Array(131, dtype=int64, weak_type=True),\n",
       " '_rvidx': array([ True,  True,  True, False, False]),\n",
       " '_rvdist': ['n', 'n', 'n'],\n",
       " 'X_raw':        ASC_SM  ASC_CAR  ASC_TRAIN    TT    CO\n",
       " 0           0        0          1  1.12  0.48\n",
       " 1           1        0          0  0.63  0.52\n",
       " 2           0        1          0  1.17  0.65\n",
       " 3           0        0          1  1.03  0.48\n",
       " 4           1        0          0  0.60  0.49\n",
       " ...       ...      ...        ...   ...   ...\n",
       " 20299       1        0          0  0.53  0.17\n",
       " 20300       0        1          0  0.80  1.04\n",
       " 20301       0        0          1  1.08  0.13\n",
       " 20302       1        0          0  0.53  0.21\n",
       " 20303       0        1          0  1.00  0.80\n",
       " \n",
       " [20304 rows x 5 columns],\n",
       " 'y_raw': 0           SM\n",
       " 1           SM\n",
       " 2           SM\n",
       " 3           SM\n",
       " 4           SM\n",
       "          ...  \n",
       " 20299    TRAIN\n",
       " 20300    TRAIN\n",
       " 20301    TRAIN\n",
       " 20302    TRAIN\n",
       " 20303    TRAIN\n",
       " Name: CHOICE, Length: 20304, dtype: object,\n",
       " 'varnames_raw': ['ASC_SM', 'ASC_CAR', 'ASC_TRAIN', 'TT', 'CO'],\n",
       " 'alts_raw': (0        TRAIN\n",
       "  1           SM\n",
       "  2          CAR\n",
       "  3        TRAIN\n",
       "  4           SM\n",
       "           ...  \n",
       "  20299       SM\n",
       "  20300      CAR\n",
       "  20301    TRAIN\n",
       "  20302       SM\n",
       "  20303      CAR\n",
       "  Name: alt, Length: 20304, dtype: object,),\n",
       " 'ids_raw': (0           0\n",
       "  1           0\n",
       "  2           0\n",
       "  3           1\n",
       "  4           1\n",
       "           ... \n",
       "  20299    6766\n",
       "  20300    6766\n",
       "  20301    6767\n",
       "  20302    6767\n",
       "  20303    6767\n",
       "  Name: custom_id, Length: 20304, dtype: int64,),\n",
       " 'randvars_raw': ({'ASC_SM': 'n', 'ASC_CAR': 'n', 'ASC_TRAIN': 'n'},),\n",
       " 'weights_raw': (None,),\n",
       " 'avail_raw': (0        1\n",
       "  1        1\n",
       "  2        1\n",
       "  3        1\n",
       "  4        1\n",
       "          ..\n",
       "  20299    1\n",
       "  20300    1\n",
       "  20301    1\n",
       "  20302    1\n",
       "  20303    1\n",
       "  Name: AV, Length: 20304, dtype: int64,),\n",
       " 'panels_raw': (None,),\n",
       " 'init_coeff_raw': (None,),\n",
       " 'maxiter_raw': (2000,),\n",
       " 'random_state_raw': (None,),\n",
       " 'n_draws_raw': (2000,),\n",
       " 'halton_raw': (True,),\n",
       " 'halton_opts_raw': (None,),\n",
       " 'tol_opts_raw': (None,),\n",
       " 'num_hess_raw': (False,),\n",
       " 'fixedvars_raw': ({'ASC_TRAIN': 0.0},),\n",
       " 'scale_factor_raw': (None,),\n",
       " 'optim_method_raw': ('trust-region',),\n",
       " 'skip_std_errs_raw': (False,),\n",
       " 'include_correlations_raw': (False,),\n",
       " 'force_positive_chol_diag_raw': (True,),\n",
       " 'hessian_by_row_raw': (True,),\n",
       " '_fit_start_time': 1752455225.7568645,\n",
       " '_varnames': [np.str_('ASC_SM'),\n",
       "  np.str_('ASC_CAR'),\n",
       "  np.str_('ASC_TRAIN'),\n",
       "  np.str_('TT'),\n",
       "  np.str_('CO')],\n",
       " 'alternatives': array(['CAR', 'SM', 'TRAIN'], dtype=object),\n",
       " 'maxiter': 2000,\n",
       " 'randvars': {'ASC_SM': 'n', 'ASC_CAR': 'n', 'ASC_TRAIN': 'n'},\n",
       " 'n_draws': 2000,\n",
       " 'convergence': Array(True, dtype=bool),\n",
       " 'grad_n': Array([[ 0.16756869, -0.09907923,  0.        , ..., -0.04410149,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.15865722, -0.07457186,  0.        , ..., -0.04950385,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.1769736 , -0.12606968,  0.        , ..., -0.03761893,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.28607813, -0.42531441,  0.        , ...,  0.0895111 ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.31124572, -0.34865329,  0.        , ...,  0.13164376,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.29414439, -0.30211123,  0.        , ...,  0.10261462,\n",
       "          0.        ,  0.        ]], dtype=float64),\n",
       " 'hess_inv': Array([[ 1.18548269e-02,  3.53583743e-03,  0.00000000e+00,\n",
       "          3.13716283e-03, -4.47078854e-03,  1.52428661e-02,\n",
       "          3.85879412e-02, -8.98699670e-04],\n",
       "        [ 3.53583743e-03,  3.37083613e-03,  0.00000000e+00,\n",
       "          1.45583029e-03, -4.24403270e-04,  2.51567243e-03,\n",
       "          9.44561476e-03,  7.58993177e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 3.13716283e-03,  1.45583029e-03,  0.00000000e+00,\n",
       "          9.36859479e-03,  3.43352616e-03, -1.04689630e-02,\n",
       "          2.82016100e-02,  7.90556880e-04],\n",
       "        [-4.47078854e-03, -4.24403270e-04,  0.00000000e+00,\n",
       "          3.43352616e-03,  1.04749391e-02, -1.59295909e-02,\n",
       "         -1.69997953e-02, -1.29608646e-03],\n",
       "        [ 1.52428661e-02,  2.51567243e-03,  0.00000000e+00,\n",
       "         -1.04689630e-02, -1.59295909e-02,  7.04303770e-02,\n",
       "          5.03487155e-02, -9.31852509e-03],\n",
       "        [ 3.85879412e-02,  9.44561476e-03,  0.00000000e+00,\n",
       "          2.82016100e-02, -1.69997953e-02,  5.03487155e-02,\n",
       "          6.83358040e+11, -3.45573876e+01],\n",
       "        [-8.98699670e-04,  7.58993177e-04,  0.00000000e+00,\n",
       "          7.90556880e-04, -1.29608646e-03, -9.31852509e-03,\n",
       "         -3.45573876e+01, -1.75582963e+13]], dtype=float64),\n",
       " 'covariance': Array([[ 1.18548269e-02,  3.53583743e-03,  0.00000000e+00,\n",
       "          3.13716283e-03, -4.47078854e-03,  1.52428661e-02,\n",
       "          3.85879412e-02, -8.98699670e-04],\n",
       "        [ 3.53583743e-03,  3.37083613e-03,  0.00000000e+00,\n",
       "          1.45583029e-03, -4.24403270e-04,  2.51567243e-03,\n",
       "          9.44561476e-03,  7.58993177e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 3.13716283e-03,  1.45583029e-03,  0.00000000e+00,\n",
       "          9.36859479e-03,  3.43352616e-03, -1.04689630e-02,\n",
       "          2.82016100e-02,  7.90556880e-04],\n",
       "        [-4.47078854e-03, -4.24403270e-04,  0.00000000e+00,\n",
       "          3.43352616e-03,  1.04749391e-02, -1.59295909e-02,\n",
       "         -1.69997953e-02, -1.29608646e-03],\n",
       "        [ 1.52428661e-02,  2.51567243e-03,  0.00000000e+00,\n",
       "         -1.04689630e-02, -1.59295909e-02,  7.04303770e-02,\n",
       "          5.03487155e-02, -9.31852509e-03],\n",
       "        [ 3.85879412e-02,  9.44561476e-03,  0.00000000e+00,\n",
       "          2.82016100e-02, -1.69997953e-02,  5.03487155e-02,\n",
       "          6.83358040e+11, -3.45573876e+01],\n",
       "        [-8.98699670e-04,  7.58993177e-04,  0.00000000e+00,\n",
       "          7.90556880e-04, -1.29608646e-03, -9.31852509e-03,\n",
       "         -3.45573876e+01, -1.75582963e+13]], dtype=float64),\n",
       " 'estimation_message': '',\n",
       " 'total_iter': Array(75, dtype=int64, weak_type=True),\n",
       " 'estim_time_sec': 57.30299687385559,\n",
       " 'sample_size': 6768,\n",
       " 'aic': Array(10524.92487202, dtype=float64),\n",
       " 'bic': Array(10572.66459833, dtype=float64),\n",
       " 'mask': Array([2], dtype=int32),\n",
       " 'fixedvars': {'ASC_TRAIN': 0.0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bounds = False\n",
    "# Array([ 1.00182804e+00,  3.83997554e-01,  0.00000000e+00, -1.77705018e+00,\n",
    "#       -1.86630446e+00, -1.54365787e-01,  3.39299844e+00, -3.04477145e-03,\n",
    "#       2.57124803e-01,  1.97240984e-01,  2.90610691e+00], dtype=float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  1.25456326,   0.75474899,   0.        ,  -1.71307838,\n",
       "        -1.76385362,   3.15994199, -24.71480277, -25.28252733],      dtype=float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rray([ 0.24806196,  0.38807779,  0.        , -2.90758556, -2.23638878,\n",
    "#        2.14518692,  1.99569859,  0.84636589])\n",
    "res['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:10:43,491 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 2000 random draws for each random variable and observation.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    betas,\n",
    "    Xdf,\n",
    "    Xdr,\n",
    "    panels,\n",
    "    draws,\n",
    "    weights,\n",
    "    avail,\n",
    "    scale_d,\n",
    "    mask,\n",
    "    values_for_mask,\n",
    "    mask_chol,\n",
    "    values_for_chol_mask,\n",
    "    rvidx,\n",
    "    rand_idx,\n",
    "    fixed_idx,\n",
    "    num_panels,\n",
    "    idx_ln_dist,\n",
    "    coef_names,\n",
    ") = model.data_prep_for_fit(\n",
    "    X=df[varnames],\n",
    "    y=df['CHOICE'],\n",
    "    varnames=varnames,\n",
    "    alts=df['alt'],\n",
    "    ids=df['custom_id'],\n",
    "    randvars=randvars,\n",
    "    weights=None,\n",
    "    avail=df['AV'],\n",
    "    panels=None if do_panel is False else df[\"ID\"],\n",
    "    init_coeff=init_vals,\n",
    "    maxiter=2000,\n",
    "    random_state=123,\n",
    "    n_draws=2000,\n",
    "    halton=True,\n",
    "    halton_opts=None,\n",
    "    fixedvars=fixedvars,\n",
    "    scale_factor=None,\n",
    "    include_correlations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([2], dtype=int32), None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask, mask_chol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([0], dtype=int32), None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_for_mask, values_for_chol_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_names[5].startswith('sd.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('sd.ASC_SM')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_names[len(model._rvidx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([0.0, 1.0], dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 18:58:13,373 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 2000 random draws for each random variable and observation.\n",
      "2025-07-11 18:58:17,673 INFO jaxlogit.mixed_logit: Shape of draws: (6768, 2, 2000), number of draws: 2000\n",
      "2025-07-11 18:58:17,674 INFO jaxlogit.mixed_logit: Shape of Xdf: (6768, 2, 3), shape of Xdr: (6768, 2, 2)\n",
      "2025-07-11 18:58:17,675 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
      "2025-07-11 18:58:17,945 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 6854.66, params= [(np.str_('ASC_SM'), Array(0., dtype=float64)), (np.str_('ASC_CAR'), Array(0., dtype=float64)), (np.str_('ASC_TRAIN'), Array(0., dtype=float64)), (np.str_('TT'), Array(0., dtype=float64)), (np.str_('CO'), Array(0., dtype=float64)), (np.str_('sd.TT'), Array(1., dtype=float64)), (np.str_('sd.CO'), Array(1., dtype=float64)), (np.str_('chol.TT.CO'), Array(1., dtype=float64))]\n",
      "2025-07-11 18:58:17,946 INFO jaxlogit._optimize: Running minimization with method trust-region\n",
      "2025-07-11 18:58:17,947 INFO jaxlogit._optimize: Running optimization with method: trust-region\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 6854.656381435868, Loss on the last accepted step: 0.0, Step size: 1.0\n",
      "Loss on this step: 1826744.4321811115, Loss on the last accepted step: 6854.656381435868, Step size: 0.25\n",
      "Loss on this step: 1273386.9212413018, Loss on the last accepted step: 6854.656381435868, Step size: 0.0625\n",
      "Loss on this step: 331837.2181703817, Loss on the last accepted step: 6854.656381435868, Step size: 0.015625\n",
      "Loss on this step: 84587.09923980582, Loss on the last accepted step: 6854.656381435868, Step size: 0.00390625\n",
      "Loss on this step: 20496.337752662046, Loss on the last accepted step: 6854.656381435868, Step size: 0.0009765625\n",
      "Loss on this step: 6342.1324213571625, Loss on the last accepted step: 6854.656381435868, Step size: 0.0009765625\n",
      "Loss on this step: 5455.370018051781, Loss on the last accepted step: 6342.1324213571625, Step size: 0.0009765625\n",
      "Loss on this step: 5349.128726099956, Loss on the last accepted step: 5455.370018051781, Step size: 0.0009765625\n",
      "Loss on this step: 5294.239061593134, Loss on the last accepted step: 5349.128726099956, Step size: 0.0009765625\n",
      "Loss on this step: 5264.604023955527, Loss on the last accepted step: 5294.239061593134, Step size: 0.0009765625\n",
      "Loss on this step: 5243.205959789728, Loss on the last accepted step: 5264.604023955527, Step size: 0.0009765625\n",
      "Loss on this step: 5230.815789215645, Loss on the last accepted step: 5243.205959789728, Step size: 0.00341796875\n",
      "Loss on this step: 5203.400876583664, Loss on the last accepted step: 5230.815789215645, Step size: 0.011962890625\n",
      "Loss on this step: 5170.998653721083, Loss on the last accepted step: 5203.400876583664, Step size: 0.011962890625\n",
      "Loss on this step: 5157.7080267502315, Loss on the last accepted step: 5170.998653721083, Step size: 0.011962890625\n",
      "Loss on this step: 5159.732415823363, Loss on the last accepted step: 5157.7080267502315, Step size: 0.00299072265625\n",
      "Loss on this step: 5154.771990917609, Loss on the last accepted step: 5157.7080267502315, Step size: 0.00299072265625\n",
      "Loss on this step: 5151.898524226149, Loss on the last accepted step: 5154.771990917609, Step size: 0.00299072265625\n",
      "Loss on this step: 5151.060428276265, Loss on the last accepted step: 5151.898524226149, Step size: 0.00299072265625\n",
      "Loss on this step: 5150.443351895692, Loss on the last accepted step: 5151.060428276265, Step size: 0.00299072265625\n",
      "Loss on this step: 5149.9518627716, Loss on the last accepted step: 5150.443351895692, Step size: 0.00299072265625\n",
      "Loss on this step: 5149.435098959167, Loss on the last accepted step: 5149.9518627716, Step size: 0.00299072265625\n",
      "Loss on this step: 5148.993294373642, Loss on the last accepted step: 5149.435098959167, Step size: 0.00299072265625\n",
      "Loss on this step: 5148.5721678770215, Loss on the last accepted step: 5148.993294373642, Step size: 0.00299072265625\n",
      "Loss on this step: 5148.224760204762, Loss on the last accepted step: 5148.5721678770215, Step size: 0.00299072265625\n",
      "Loss on this step: 5147.877912578097, Loss on the last accepted step: 5148.224760204762, Step size: 0.00299072265625\n",
      "Loss on this step: 5147.579277792851, Loss on the last accepted step: 5147.877912578097, Step size: 0.00299072265625\n",
      "Loss on this step: 5147.279652369198, Loss on the last accepted step: 5147.579277792851, Step size: 0.00299072265625\n",
      "Loss on this step: 5147.023215595743, Loss on the last accepted step: 5147.279652369198, Step size: 0.00299072265625\n",
      "Loss on this step: 5146.763627003363, Loss on the last accepted step: 5147.023215595743, Step size: 0.00299072265625\n",
      "Loss on this step: 5146.539344499339, Loss on the last accepted step: 5146.763627003363, Step size: 0.00299072265625\n",
      "Loss on this step: 5146.310974491498, Loss on the last accepted step: 5146.539344499339, Step size: 0.00299072265625\n",
      "Loss on this step: 5146.10891996026, Loss on the last accepted step: 5146.310974491498, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.912752405151, Loss on the last accepted step: 5146.10891996026, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.72642211791, Loss on the last accepted step: 5145.912752405151, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.547302779416, Loss on the last accepted step: 5145.72642211791, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.375248226673, Loss on the last accepted step: 5145.547302779416, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.21002946001, Loss on the last accepted step: 5145.375248226673, Step size: 0.00299072265625\n",
      "Loss on this step: 5145.051475184918, Loss on the last accepted step: 5145.21002946001, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.8993733181715, Loss on the last accepted step: 5145.051475184918, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.753635645098, Loss on the last accepted step: 5144.8993733181715, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.614148777645, Loss on the last accepted step: 5144.753635645098, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.48101614955, Loss on the last accepted step: 5144.614148777645, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.354397037543, Loss on the last accepted step: 5144.48101614955, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.234889514329, Loss on the last accepted step: 5144.354397037543, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.123484727214, Loss on the last accepted step: 5144.234889514329, Step size: 0.00299072265625\n",
      "Loss on this step: 5144.022289210111, Loss on the last accepted step: 5144.123484727214, Step size: 0.00299072265625\n",
      "Loss on this step: 5143.934814969005, Loss on the last accepted step: 5144.022289210111, Step size: 0.010467529296875\n",
      "Loss on this step: 5143.704894550042, Loss on the last accepted step: 5143.934814969005, Step size: 0.0366363525390625\n",
      "Loss on this step: 5143.4075231762545, Loss on the last accepted step: 5143.704894550042, Step size: 0.0366363525390625\n",
      "Loss on this step: 5143.034180196235, Loss on the last accepted step: 5143.4075231762545, Step size: 0.0366363525390625\n",
      "Loss on this step: 5142.873445278484, Loss on the last accepted step: 5143.034180196235, Step size: 0.0366363525390625\n",
      "Loss on this step: 5142.646081677195, Loss on the last accepted step: 5142.873445278484, Step size: 0.0366363525390625\n",
      "Loss on this step: 5142.757726080641, Loss on the last accepted step: 5142.646081677195, Step size: 0.009159088134765625\n",
      "Loss on this step: 5142.569801496691, Loss on the last accepted step: 5142.646081677195, Step size: 0.009159088134765625\n",
      "Loss on this step: 5142.505295762749, Loss on the last accepted step: 5142.569801496691, Step size: 0.009159088134765625\n",
      "Loss on this step: 5142.464919813474, Loss on the last accepted step: 5142.505295762749, Step size: 0.03205680847167969\n",
      "Loss on this step: 5142.333613637533, Loss on the last accepted step: 5142.464919813474, Step size: 0.1121988296508789\n",
      "Loss on this step: 5142.901645765556, Loss on the last accepted step: 5142.333613637533, Step size: 0.028049707412719727\n",
      "Loss on this step: 5142.186072822127, Loss on the last accepted step: 5142.333613637533, Step size: 0.09817397594451904\n",
      "Loss on this step: 5142.879436309919, Loss on the last accepted step: 5142.186072822127, Step size: 0.02454349398612976\n",
      "Loss on this step: 5142.034289410212, Loss on the last accepted step: 5142.186072822127, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.922537157635, Loss on the last accepted step: 5142.034289410212, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.9013605441005, Loss on the last accepted step: 5141.922537157635, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.738166356299, Loss on the last accepted step: 5141.9013605441005, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.660622391387, Loss on the last accepted step: 5141.738166356299, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.594756305511, Loss on the last accepted step: 5141.660622391387, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.5233576747305, Loss on the last accepted step: 5141.594756305511, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.455070351795, Loss on the last accepted step: 5141.5233576747305, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.392378178968, Loss on the last accepted step: 5141.455070351795, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.363154220161, Loss on the last accepted step: 5141.392378178968, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.339693748176, Loss on the last accepted step: 5141.363154220161, Step size: 0.02454349398612976\n",
      "Loss on this step: 5141.306657016489, Loss on the last accepted step: 5141.339693748176, Step size: 0.08590222895145416\n",
      "Loss on this step: 5141.207152703217, Loss on the last accepted step: 5141.306657016489, Step size: 0.08590222895145416\n",
      "Loss on this step: 5141.079973009922, Loss on the last accepted step: 5141.207152703217, Step size: 0.08590222895145416\n",
      "Loss on this step: 5141.051203809474, Loss on the last accepted step: 5141.079973009922, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.994110663626, Loss on the last accepted step: 5141.051203809474, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.935910234103, Loss on the last accepted step: 5140.994110663626, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.899006298325, Loss on the last accepted step: 5140.935910234103, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.867863184143, Loss on the last accepted step: 5140.899006298325, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.835468955517, Loss on the last accepted step: 5140.867863184143, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.809228888165, Loss on the last accepted step: 5140.835468955517, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.791189753266, Loss on the last accepted step: 5140.809228888165, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.772128881365, Loss on the last accepted step: 5140.791189753266, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.759722563207, Loss on the last accepted step: 5140.772128881365, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.751108682041, Loss on the last accepted step: 5140.759722563207, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.744173857635, Loss on the last accepted step: 5140.751108682041, Step size: 0.08590222895145416\n",
      "Loss on this step: 5140.738387537246, Loss on the last accepted step: 5140.744173857635, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.726119215366, Loss on the last accepted step: 5140.738387537246, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.717096543791, Loss on the last accepted step: 5140.726119215366, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.7125704820055, Loss on the last accepted step: 5140.717096543791, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.71063855294, Loss on the last accepted step: 5140.7125704820055, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.71029702985, Loss on the last accepted step: 5140.71063855294, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.709872530579, Loss on the last accepted step: 5140.71029702985, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.709742288348, Loss on the last accepted step: 5140.709872530579, Step size: 0.30065780133008957\n",
      "Loss on this step: 5140.709695988553, Loss on the last accepted step: 5140.709742288348, Step size: 1.0523023046553135\n",
      "Loss on this step: 5140.709656210992, Loss on the last accepted step: 5140.709695988553, Step size: 1.0523023046553135\n",
      "Loss on this step: 5140.709655920643, Loss on the last accepted step: 5140.709656210992, Step size: 1.0523023046553135\n",
      "Loss on this step: 5140.709654601758, Loss on the last accepted step: 5140.709655920643, Step size: 1.0523023046553135\n",
      "Loss on this step: 5140.709654598368, Loss on the last accepted step: 5140.709654601758, Step size: 1.0523023046553135\n",
      "Loss on this step: 5140.709654590151, Loss on the last accepted step: 5140.709654598368, Step size: 3.683058066293597\n",
      "Loss on this step: 5140.709654590149, Loss on the last accepted step: 5140.709654590151, Step size: 12.89070323202759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 18:59:10,190 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -5140.71\n",
      "2025-07-11 18:59:10,192 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n",
      "2025-07-11 18:59:16,533 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
      "2025-07-11 18:59:21,251 INFO jaxlogit._choice_model: Post fit processing\n"
     ]
    }
   ],
   "source": [
    "res = model.fit(\n",
    "    X=df[varnames],\n",
    "    y=df['CHOICE'],\n",
    "    varnames=varnames,\n",
    "    alts=df['alt'],\n",
    "    ids=df['custom_id'],\n",
    "    avail=df['AV'],\n",
    "    panels=None if do_panel is False else df[\"ID\"],\n",
    "    randvars=randvars,\n",
    "    n_draws=2000,\n",
    "    fixedvars=fixedvars,\n",
    "    init_coeff=init_vals,\n",
    "    include_correlations=True,  # Enable correlation between random parameters\n",
    "    use_bounds=False,\n",
    "    optim_method='trust-region',  # 'L-BFGS-B'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:57:03.025229Z",
     "iopub.status.busy": "2025-07-08T19:57:03.024862Z",
     "iopub.status.idle": "2025-07-08T19:57:06.900847Z",
     "shell.execute_reply": "2025-07-08T19:57:06.899726Z",
     "shell.execute_reply.started": "2025-07-08T19:57:03.025204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 18:59:21,284 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 1000 random draws for each random variable and observation.\n"
     ]
    }
   ],
   "source": [
    "prosb = model.predict(\n",
    "    X=df[varnames],\n",
    "    varnames=varnames,\n",
    "    init_coeff=res['x'],\n",
    "    alts=df['alt'],\n",
    "    ids=df['custom_id'],\n",
    "    avail=df['AV'],\n",
    "    panels=None if do_panel is False else df[\"ID\"],\n",
    "    randvars=randvars,\n",
    "    n_draws=1000,\n",
    "    include_correlations=True,  # Enable correlation between random parameters\n",
    ")\n",
    "\n",
    "# these have to be in same order for this to work - enforced in long_format consistency check, use it here\n",
    "uq_alts, idx = np.unique(df['alt'], return_index=True)\n",
    "uq_alts = uq_alts[np.argsort(idx)]\n",
    "\n",
    "\n",
    "probs = pd.DataFrame(prosb, columns=uq_alts, index=np.unique(df['custom_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:57:12.702150Z",
     "iopub.status.busy": "2025-07-08T19:57:12.700708Z",
     "iopub.status.idle": "2025-07-08T19:57:12.713134Z",
     "shell.execute_reply": "2025-07-08T19:57:12.711300Z",
     "shell.execute_reply.started": "2025-07-08T19:57:12.702109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CHOICE\n",
       "SM       0.604314\n",
       "CAR      0.261525\n",
       "TRAIN    0.134161\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wide.CHOICE.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:57:14.349432Z",
     "iopub.status.busy": "2025-07-08T19:57:14.348676Z",
     "iopub.status.idle": "2025-07-08T19:57:14.360593Z",
     "shell.execute_reply": "2025-07-08T19:57:14.358832Z",
     "shell.execute_reply.started": "2025-07-08T19:57:14.349393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRAIN    0.131201\n",
       "SM       0.602500\n",
       "CAR      0.266299\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T19:57:16.936938Z",
     "iopub.status.busy": "2025-07-08T19:57:16.935832Z",
     "iopub.status.idle": "2025-07-08T19:57:17.942216Z",
     "shell.execute_reply": "2025-07-08T19:57:17.940682Z",
     "shell.execute_reply.started": "2025-07-08T19:57:16.936891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'TRAIN'}>,\n",
       "        <Axes: title={'center': 'SM'}>],\n",
       "       [<Axes: title={'center': 'CAR'}>, <Axes: >]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV CORNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 04:12:51,575 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 1000 random draws for each random variable and observation.\n",
      "2025-07-08 04:12:54,800 INFO jaxlogit: Shape of draws: (6768, 3, 1000), number of draws: 1000\n",
      "2025-07-08 04:12:54,801 INFO jaxlogit: Shape of Xdf: (6768, 2, 2), shape of Xdr: (6768, 2, 3)\n",
      "2025-07-08 04:12:54,802 INFO jaxlogit: Compiling log-likelihood function.\n",
      "2025-07-08 04:12:55,289 INFO jaxlogit: Compilation finished, init neg_loglike = 6964.58, params= [(np.str_('ASC_SM'), Array(0., dtype=float64)), (np.str_('ASC_CAR'), Array(0., dtype=float64)), (np.str_('ASC_TRAIN'), Array(0., dtype=float64)), (np.str_('CO'), Array(0., dtype=float64)), (np.str_('TT'), Array(0., dtype=float64)), (np.str_('sd.ASC_SM'), Array(1., dtype=float64)), (np.str_('sd.ASC_CAR'), Array(1., dtype=float64)), (np.str_('sd.ASC_TRAIN'), Array(1., dtype=float64))]\n"
     ]
    }
   ],
   "source": [
    "X=df[varnames]\n",
    "y=df['CHOICE']\n",
    "varnames=varnames\n",
    "alts=df['alt']\n",
    "ids=df['custom_id']\n",
    "avail=df['AV']\n",
    "panels=None if do_panel is False else df[\"ID\"]\n",
    "randvars=randvars\n",
    "n_draws=1000\n",
    "fixedvars=fixedvars\n",
    "init_coeff=init_vals\n",
    "include_correlations=False\n",
    "use_bounds=False\n",
    "# defaults\n",
    "weights=None\n",
    "maxiter=2000\n",
    "random_state=None\n",
    "halton=True\n",
    "halton_opts=None\n",
    "fixedvars=None\n",
    "scale_factor=None\n",
    "\n",
    "(\n",
    "    betas,\n",
    "    Xdf,\n",
    "    Xdr,\n",
    "    panels,\n",
    "    draws,\n",
    "    weights,\n",
    "    avail,\n",
    "    scale_d,\n",
    "    mask,\n",
    "    values_for_mask,\n",
    "    rvidx,\n",
    "    rand_idx,\n",
    "    fixed_idx,\n",
    "    num_panels,\n",
    "    idx_ln_dist,\n",
    "    coef_names,\n",
    ") = model.data_prep_for_fit(\n",
    "    X,\n",
    "    y,\n",
    "    varnames,\n",
    "    alts,\n",
    "    ids,\n",
    "    randvars,\n",
    "    weights=weights,\n",
    "    avail=avail,\n",
    "    panels=panels,\n",
    "    init_coeff=init_coeff,\n",
    "    maxiter=maxiter,\n",
    "    random_state=random_state,\n",
    "    n_draws=n_draws,\n",
    "    halton=halton,\n",
    "    halton_opts=halton_opts,\n",
    "    fixedvars=fixedvars,\n",
    "    scale_factor=scale_factor,\n",
    "    include_correlations=include_correlations,\n",
    ")\n",
    "\n",
    "fargs = (\n",
    "    Xdf,\n",
    "    Xdr,\n",
    "    panels,\n",
    "    draws,\n",
    "    weights,\n",
    "    avail,\n",
    "    scale_d,\n",
    "    mask,\n",
    "    values_for_mask,\n",
    "    rvidx,\n",
    "    rand_idx,\n",
    "    fixed_idx,\n",
    "    num_panels,\n",
    "    idx_ln_dist,\n",
    "    include_correlations,\n",
    ")\n",
    "\n",
    "import logging\n",
    "import jaxlogit\n",
    "logger = logging.getLogger(\"jaxlogit\")\n",
    "\n",
    "if idx_ln_dist.shape[0] > 0:\n",
    "    logger.info(\n",
    "        f\"Lognormal distributions found for {idx_ln_dist.shape[0]} random variables, applying transformation.\"\n",
    "    )\n",
    "\n",
    "if scale_d is not None:\n",
    "    logger.info(\"Scaling is in use, scaling the data by the scale factor.\")\n",
    "\n",
    "if panels is not None:\n",
    "    logger.info(f\"Data contains {num_panels} panels, using segment_sum for panel-wise log-likelihood.\")\n",
    "\n",
    "logger.info(f\"Shape of draws: {draws.shape}, number of draws: {n_draws}\")\n",
    "logger.info(f\"Shape of Xdf: {Xdf.shape}, shape of Xdr: {Xdr.shape}\")\n",
    "\n",
    "logger.info(\"Compiling log-likelihood function.\")\n",
    "jit_neg_loglike = jax.jit(jaxlogit.mixed_logit.neg_loglike, static_argnames=[\"num_panels\", \"include_correlations\"])\n",
    "neg_loglik_and_grad = jax.value_and_grad(jit_neg_loglike, argnums=0)\n",
    "init_loglik = neg_loglik_and_grad(betas, *fargs)\n",
    "logger.info(\n",
    "    f\"Compilation finished, init neg_loglike = {init_loglik[0]:.2f}, params= {list(zip(coef_names, betas))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_start_idx = len(rvidx)\n",
    "sd_slice_size = len(rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.str_('ASC_SM'),\n",
       "  np.str_('ASC_CAR'),\n",
       "  np.str_('ASC_TRAIN'),\n",
       "  np.str_('CO'),\n",
       "  np.str_('TT')],\n",
       " Array([ True,  True,  True, False, False], dtype=bool),\n",
       " array(['ASC_SM', 'ASC_CAR', 'ASC_TRAIN', 'CO', 'TT', 'sd.ASC_SM',\n",
       "        'sd.ASC_CAR', 'sd.ASC_TRAIN'], dtype='<U12'),\n",
       " Array([0, 1, 2], dtype=int64),\n",
       " Array([3, 4], dtype=int64),\n",
       " 5,\n",
       " 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._varnames, rvidx, coef_names, rand_idx, fixed_idx, sd_start_idx, sd_slice_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1., 1.], dtype=float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.lax.dynamic_slice(\n",
    "    betas, (sd_start_idx,), (sd_slice_size,)\n",
    ")  # Extract the std devs from the betas vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUP</th>\n",
       "      <th>SURVEY</th>\n",
       "      <th>SP</th>\n",
       "      <th>ID</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>FIRST</th>\n",
       "      <th>TICKET</th>\n",
       "      <th>WHO</th>\n",
       "      <th>LUGGAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN_CO</th>\n",
       "      <th>TRAIN_HE</th>\n",
       "      <th>SM_TT</th>\n",
       "      <th>SM_CO</th>\n",
       "      <th>SM_HE</th>\n",
       "      <th>SM_SEATS</th>\n",
       "      <th>CAR_TT</th>\n",
       "      <th>CAR_CO</th>\n",
       "      <th>CHOICE</th>\n",
       "      <th>custom_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>120</td>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>65</td>\n",
       "      <td>SM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GROUP  SURVEY  SP  ID  PURPOSE  FIRST  TICKET  WHO  LUGGAGE  AGE  ...  \\\n",
       "0      2       0   1   1        1      0       1    1        0    3  ...   \n",
       "\n",
       "   TRAIN_CO  TRAIN_HE  SM_TT  SM_CO  SM_HE  SM_SEATS  CAR_TT  CAR_CO  CHOICE  \\\n",
       "0        48       120     63     52     20         0     117      65      SM   \n",
       "\n",
       "   custom_id  \n",
       "0          0  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wide.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.04,  0.49],\n",
       "       [ 0.13,  0.54]], dtype=float64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.,  0.,  1.],\n",
       "       [-1.,  1.,  0.]], dtype=float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdr[0]  # first one: -asccar + asc_train, second one -asc_car + asc_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   ASC_SM  ASC_CAR  ASC_TRAIN    CO    TT\n",
       " 0       0        0          1  0.48  1.12\n",
       " 1       1        0          0  0.52  0.63\n",
       " 2       0        1          0  0.65  1.17,\n",
       " 0    SM\n",
       " 1    SM\n",
       " 2    SM\n",
       " Name: CHOICE, dtype: object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3), y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.,  0., -1.],\n",
       "       [ 0.,  1., -1.]], dtype=float64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdr[-1]  # so first one: asc_car - asc_train, second one: asc_sm - asc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUP</th>\n",
       "      <th>SURVEY</th>\n",
       "      <th>SP</th>\n",
       "      <th>ID</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>FIRST</th>\n",
       "      <th>TICKET</th>\n",
       "      <th>WHO</th>\n",
       "      <th>LUGGAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN_CO</th>\n",
       "      <th>TRAIN_HE</th>\n",
       "      <th>SM_TT</th>\n",
       "      <th>SM_CO</th>\n",
       "      <th>SM_HE</th>\n",
       "      <th>SM_SEATS</th>\n",
       "      <th>CAR_TT</th>\n",
       "      <th>CAR_CO</th>\n",
       "      <th>CHOICE</th>\n",
       "      <th>custom_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>939</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>6767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GROUP  SURVEY  SP   ID  PURPOSE  FIRST  TICKET  WHO  LUGGAGE  AGE  ...  \\\n",
       "8450      3       1   1  939        3      1       7    3        1    5  ...   \n",
       "\n",
       "      TRAIN_CO  TRAIN_HE  SM_TT  SM_CO  SM_HE  SM_SEATS  CAR_TT  CAR_CO  \\\n",
       "8450        13        60     53     21     30         0     100      80   \n",
       "\n",
       "      CHOICE  custom_id  \n",
       "8450   TRAIN       6767  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wide.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAR', 'SM', 'TRAIN'], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cov dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "tril_rows, tril_cols = jnp.tril_indices(4)\n",
    "diag_mask = tril_rows == tril_cols\n",
    "off_diag_mask = ~diag_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ True, False,  True, False, False,  True, False, False, False,\n",
       "         True], dtype=bool),\n",
       " Array([False,  True, False,  True,  True, False,  True,  True,  True,\n",
       "        False], dtype=bool),\n",
       " Array([0, 1, 1, 2, 2, 2, 3, 3, 3, 3], dtype=int64),\n",
       " Array([0, 0, 1, 0, 1, 2, 0, 1, 2, 3], dtype=int64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_mask, off_diag_mask, tril_rows, tril_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_vals = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "off_diag_vals = jnp.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1,  0,  0,  1,  2,  2,  3,  4,  5,  5], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.cumsum(off_diag_mask) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.6, 0.1, 0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.6, 0.6], dtype=float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_diag_vals[jnp.cumsum(off_diag_mask) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_vals = jnp.where(\n",
    "    diag_mask,\n",
    "    diag_vals[tril_rows],  # diagonal values\n",
    "    off_diag_vals[jnp.cumsum(off_diag_mask) - 1]  # off-diagonal values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1. , 0.1, 2. , 0.2, 0.3, 3. , 0.4, 0.5, 0.6, 4. ], dtype=float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = jnp.zeros((4, 4))\n",
    "L = L.at[tril_rows, tril_cols].set(tril_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1. , 0. , 0. , 0. ],\n",
       "       [0.1, 2. , 0. , 0. ],\n",
       "       [0.2, 0.3, 3. , 0. ],\n",
       "       [0.4, 0.5, 0.6, 4. ]], dtype=float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we fill in tril indeces by counting from left to right, then top to bottom (fill order of indexes: 10, 20, 21, 30, 31, 32, 40, 41, 42, 43)\n",
    "# in order to set the first column to zero and the first diagonal value of the second column to 1, we need to order the ascs in the dataframe (input order is order of processing)\n",
    "# we also need to ensure that the elements are named in the order as tril indexes are filled, so we can fix the values by name.\n",
    "# I think the naming is not consistent yet.\n",
    "# How about I turn each dev notebook into an example though?\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at matrix multiplication - is this correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws have shape (num_obs, num_random_vars, num_draws).\n",
    "# for non-correlated rans: each var is multiplied by diagonal, across both draws and obs\n",
    "# for correlated vars: each var is multiplied by a cholesky matrix, across both draws and obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = np.ones((4, 3, 5))\n",
    "draws[1,:,:] = np.array([2.,2.,2.,2.,2.])\n",
    "\n",
    "diag_vals = np.array([1.0, 2.0, 3.0])\n",
    "L = np.array(\n",
    "    [[1.0, 0.0, 0.0],\n",
    "     [0.1, 2.0, 0.0],\n",
    "     [0.2, 0.3, 3.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]],\n",
       "\n",
       "       [[2., 2., 2., 2., 2.],\n",
       "        [4., 4., 4., 4., 4.],\n",
       "        [6., 6., 6., 6., 6.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draws * diag_vals[None, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1. , 2.1, 3.5]), array([2. , 4.2, 7. ]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L @ draws[0,:,0], L @ draws[1,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, num_rands, R = draws.shape\n",
    "draws_flat = draws.transpose(0, 2, 1).reshape(-1, num_rands)\n",
    "correlated_flat = (L @ draws_flat.T).T\n",
    "cov = correlated_flat.reshape(N, R, num_rands).transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1. , 1. , 1. , 1. , 1. ],\n",
       "        [2.1, 2.1, 2.1, 2.1, 2.1],\n",
       "        [3.5, 3.5, 3.5, 3.5, 3.5]],\n",
       "\n",
       "       [[2. , 2. , 2. , 2. , 2. ],\n",
       "        [4.2, 4.2, 4.2, 4.2, 4.2],\n",
       "        [7. , 7. , 7. , 7. , 7. ]],\n",
       "\n",
       "       [[1. , 1. , 1. , 1. , 1. ],\n",
       "        [2.1, 2.1, 2.1, 2.1, 2.1],\n",
       "        [3.5, 3.5, 3.5, 3.5, 3.5]],\n",
       "\n",
       "       [[1. , 1. , 1. , 1. , 1. ],\n",
       "        [2.1, 2.1, 2.1, 2.1, 2.1],\n",
       "        [3.5, 3.5, 3.5, 3.5, 3.5]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This seems to work, now fix naming convention in chol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder = Xnames[_rvidx]\n",
    "placeholder = ['var1', 'var2', 'var3', 'var4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stuff', 'chol.var1.var2', 'chol.var1.var3', 'chol.var2.var3',\n",
       "       'chol.var1.var4', 'chol.var2.var4', 'chol.var3.var4'], dtype='<U14')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_names = np.array(['stuff'])\n",
    "\n",
    "corr_names = [\n",
    "    f\"chol.{i}.{j}\" for idx_j, j in enumerate(placeholder) for i in placeholder[:idx_j]\n",
    "]\n",
    "coef_names = np.append(coef_names, corr_names)\n",
    "coef_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order should be \n",
    "# 'chol.var1.var2', \n",
    "# 'chol.var1.var3', 'chol.var2.var3',\n",
    "# 'chol.var1.var4', 'chol.var2.var4', 'chol.var3.var4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mixed_logit_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
